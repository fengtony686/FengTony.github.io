---
layout:     post
title:      A Survey towards Causality in Machine Learning
subtitle:   Final Survey of Introduction to Artificial Intelligence Class
date:       2020-01-08
author:     Tony Feng
header-img: img/kirin.jpg
catalog: true
tags:
    - 论文
    - 学习
    - CS
---

# Abstract
Causality is an important concept in machine learning. However, it has not been attached enough importance. However, in the future, artiﬁcial intelligence can not be merely curve ﬁtting and stay at the level of association. Causality models will be a signiﬁcant step to achieve strong AI. In this survey, the deﬁnition of causality will be introduced at ﬁrst, including three levels of causality and the diﬀerence between event causality and procedure causality. In the third section, a model called SCM which is the most complete causal model at present will be introduced. Then we will demonstrate how can it solve some of the problems in machine learning. A simple approach to the causal loop diagram will be made in the ﬁfth part. Although researches towards causality is not focused and many parts need to be improved, many remain optimistic about the development of causality. 

# A Survey towards Causality in Machine Learning
[**Download PDF**](https://www.wuhanhour.icu/essay/survey_of_causality.pdf)(Please wait for a few seconds)\\
[**BibTex**](https://www.wuhanhour.icu/essay/survey_of_causality.txt)

# References
Kenneth Appel and Wolfgang Haken. Every planar map is four colorable. Bulletin of the American mathematical Society, 82(5):711–712, 1976.

Thomas Bayes. Lii. an essay towards solving a problem in the doctrine of chances. by the late rev. mr. bayes, frs communicated by mr. price, in a letter to john canton, amfr s. Philosophical transactions of the Royal Society of London, (53):370–418, 1763.

Michel Besserve, R´emy Sun, and Bernhard Sch¨olkopf. Counterfactuals uncover the modular structure of deep generative models. arXiv preprint arXiv:1812.03253, 2018.

Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Fast conditional independence test for vector variables with large sample sizes. arXiv preprint arXiv:1804.02747, 2018.

Povilas Daniusis, Dominik Janzing, Joris Mooij, Jakob Zscheischler, Bastian Steudel, Kun Zhang, and Bernhard Sch¨olkopf. Inferring deterministic causal relations. arXiv preprint arXiv:1203.3475, 2012.

AP Dempster. Causality and statistics. Journal of statistical planning and inference, 25(3): 261–278, 1990.

Georges Dicker. Hume’s epistemology and metaphysics: an introduction. Routledge, 2002.

Andrew Forney. A Framework for Empirical Counterfactuals, or for All Intents, a Purpose. PhD thesis, University of California, Los Angeles, 2018.

Andrew Forney, Judea Pearl, and Elias Bareinboim. Counterfactual data-fusion for online reinforcement learners. In Doina Precup and Yee Whye Teh, editors, Proceedings of the
34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1156–1164, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR. URL http://proceedings.mlr.press/v70/forney17a.html.

Dan Geiger, Thomas Verma, and Judea Pearl. d-separation: From theorems to algorithms. In Machine Intelligence and Pattern Recognition, volume 10, pages 139–148. Elsevier, 1990.

Brett R Gordon, Florian Zettelmeyer, Neha Bhargava, and Dan Chapsky. A comparison of approaches to advertising measurement: Evidence from big ﬁeld experiments at facebook. Marketing Science, 38(2):193–225, 2019.

Kevin Hartnett. To build truly intelligent machines, teach them cause and effect. Internet: https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-themcause-and-eﬀect-20180515/, 2018.

Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 81(396):945–960, 1986.

Patrik O. Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Sch¨olkopf. Nonlinear causal discovery with additive noise models. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 689–696. Curran Associates, Inc., 2009. URL http://papers.nips.cc/paper/ 3548-nonlinear-causal-discovery-with-additive-noise-models.pdf.

Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. arXiv preprint arXiv:1905.02175, 2019.

K Ming Leung. Naive bayesian classiﬁer. Polytechnic University Department of Computer Science/Finance and Risk Engineering, 2007.

Chaochao Lu, Bernhard Sch¨olkopf, and Jos´e Miguel Hern´andez-Lobato. Deconfounding reinforcement learning in observational settings. arXiv preprint arXiv:1812.10576, 2018.

Marloes H Maathuis, Diego Colombo, et al. A generalized back-door criterion. The Annals of Statistics, 43(3):1060–1088, 2015.

Dominique Drouet Mari and Samuel Kotz. Correlation and dependence. World Scientiﬁc, 2001.

Judea Pearl. Giving computers free will. Internet: https://www.forbes.com/2009/06/18/computers-free-will-opinions-contributorsartiﬁcial-intelligence-09-judea-pearl.html, 2009a.

Judea Pearl. Causality. Cambridge university press, 2009b.

Judea Pearl. Bayesian networks. 2011.

Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Commun. ACM, 62(3):54–60, 2019.

George P Richardson. Problems with causal-loop diagrams. System dynamics review, 2(2): 158–170, 1986.

Bernhard Sch¨olkopf. Causality for Machine Learning. arXiv e-prints, art. arXiv:1911.10500, Nov 2019.

Bernhard Sch¨olkopf, Dominik Janzing, Jonas Peters, and Kun Zhang. Robust Learning via Cause-Eﬀect Models. arXiv e-prints, art. arXiv:1112.2738, Dec 2011.

Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the ﬁrst adversarially robust neural network model on mnist. arXiv preprint arXiv:1805.09190, 2018.

Jasjeet S Sekhon. The neyman-rubin model of causal inference and estimation via matching methods. The Oxford handbook of political methodology, 2:1–citation lastpage, 2008.

Shohei Shimizu, Patrik O Hoyer, Aapo Hyv¨arinen, and Antti Kerminen. A linear nongaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7 (Oct):2003–2030, 2006.

Robert R Tucci. Introduction to judea pearl’s do-calculus. arXiv preprint arXiv:1305.5506, 2013.


